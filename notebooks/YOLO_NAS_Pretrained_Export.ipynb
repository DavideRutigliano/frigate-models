{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "LFrYuNXjfW2Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rmuF9iKWTbdk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!git clone https://github.com/Deci-AI/super-gradients.git /content/super_gradients_folder\n",
        "! sed -i 's/sghub.deci.ai/sg-hub-nv.s3.amazonaws.com/' /content/super_gradients_folder/src/super_gradients/training/pretrained_models.py\n",
        "! sed -i 's/sghub.deci.ai/sg-hub-nv.s3.amazonaws.com/' /content/super_gradients_folder/src/super_gradients/training/utils/checkpoint_utils.py\n",
        "!pip install -e /content/super_gradients_folder\n",
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select model"
      ],
      "metadata": {
        "id": "32ZL1W_3fbcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'yolonas_s' #@param [\"yolonas_s\", \"yolonas_m\", \"yolonas_l\"]\n",
        "quantization = 'uint8' #@param [\"fp16\", \"uint8\"]\n",
        "input_width = 320 #@param {type:\"slider\", min:320, max:640, step:320}\n",
        "input_height = 320 #@param {type:\"slider\", min:320, max:640, step:320}\n",
        "\n",
        "MODEL_FILENAME = f\"{model_name}_{quantization}_{input_width}x{input_height}.onnx\""
      ],
      "metadata": {
        "id": "O_f9J1KAOdGU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dTB0jy_NNSFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b477368b-5b5f-436b-9e74-6f341bd49eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The console stream is logged into /root/sg_logs/console.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2025-03-13 19:42:59] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
            "[2025-03-13 19:43:10] INFO - utils.py - NumExpr defaulting to 2 threads.\n",
            "DEBUG:2025-03-13 19:43:16,401:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n",
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "[2025-03-13 19:43:21] WARNING - checkpoint_utils.py - :warning: The pre-trained models provided by SuperGradients may have their own licenses or terms and conditions derived from the dataset used for pre-training.\n",
            " It is your responsibility to determine whether you have permission to use the models for your use case.\n",
            " The model you have requested was pre-trained on the coco dataset, published under the following terms: https://cocodataset.org/#termsofuse\n",
            "[2025-03-13 19:43:21] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
            "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\n",
            "By downloading the pre-trained weight files you agree to comply with these terms.\n",
            "[2025-03-13 19:43:22] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_s\n"
          ]
        }
      ],
      "source": [
        "from super_gradients.common.object_names import Models\n",
        "from super_gradients.training import models\n",
        "\n",
        "MODEL_NAMES = {\n",
        "    \"yolonas_s\": Models.YOLO_NAS_S,\n",
        "    \"yolonas_m\": Models.YOLO_NAS_M,\n",
        "    \"yolonas_l\": Models.YOLO_NAS_L,\n",
        "}\n",
        "\n",
        "model = models.get(MODEL_NAMES[model_name], pretrained_weights=\"coco\")\n",
        "model.eval()\n",
        "model.prep_model_for_conversion(input_size=[1, 3, input_height, input_width])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GymUghyCNXem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c3293a-2a99-49b1-b2b5-c3ef3b448a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2025-03-13 19:43:46] WARNING - tensor_quantizer.py - Use Pytorch's native experimental fake quantization.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0313 19:43:46.409715 140049008185344 tensor_quantizer.py:281] Use Pytorch's native experimental fake quantization.\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:284: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if amax.numel() == 1:\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:286: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  inputs, amax.item() / bound, 0,\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_quantization/utils/reduce_amax.py:61: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if not keepdims or output.numel() == 1:\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:292: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0])\n",
            "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model exported successfully to yolonas_s_uint8_320x320.onnx\n",
              "Model expects input image of shape [1, 3, 320, 320]\n",
              "Input image dtype is torch.uint8\n",
              "Exported model already contains preprocessing (normalization) step, so you don't need to do it manually.\n",
              "Preprocessing steps to be applied to input image are:\n",
              "Sequential(\n",
              "  (0): CastTensorTo(dtype=torch.float32)\n",
              "  (1): ApplyMeanStd(mean=[0.], scale=[255.])\n",
              ")\n",
              "\n",
              "Exported model contains postprocessing (NMS) step with the following parameters:\n",
              "    num_pre_nms_predictions=1000\n",
              "    max_predictions_per_image=20\n",
              "    nms_threshold=0.7\n",
              "    confidence_threshold=0.4\n",
              "    output_predictions_format=DetectionOutputFormatMode.FLAT_FORMAT\n",
              "\n",
              "Exported model is in ONNX format and can be used with ONNXRuntime\n",
              "To run inference with ONNXRuntime, please use the following code snippet:\n",
              "\n",
              "    import onnxruntime\n",
              "    import numpy as np\n",
              "    session = onnxruntime.InferenceSession(\"yolonas_s_uint8_320x320.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
              "    inputs = [o.name for o in session.get_inputs()]\n",
              "    outputs = [o.name for o in session.get_outputs()]\n",
              "    example_input_image = np.zeros((1, 3, 320, 320)).astype(np.uint8)\n",
              "    predictions = session.run(outputs, {inputs[0]: example_input_image})\n",
              "\n",
              "Exported model has predictions in DetectionOutputFormatMode.FLAT_FORMAT format:\n",
              "\n",
              "    # flat_predictions is a 2D array of [N,7] shape\n",
              "    # Each row represents (image_index, x_min, y_min, x_max, y_max, confidence, class_id)\n",
              "    # Please note all values are floats, so you have to convert them to integers if needed\n",
              "    [flat_predictions] = predictions\n",
              "    for (_, x_min, y_min, x_max, y_max, confidence, class_id) in flat_predictions[0]:\n",
              "        class_id = int(class_id)\n",
              "        print(f\"Detected object with class_id={class_id}, confidence={confidence}, x_min={x_min}, y_min={y_min}, x_max={x_max}, y_max={y_max}\")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from super_gradients.conversion import DetectionOutputFormatMode\n",
        "from super_gradients.conversion.conversion_enums import ExportQuantizationMode\n",
        "\n",
        "quantization_mode = ExportQuantizationMode.FP16 if dtype == \"fp16\" else ExportQuantizationMode.INT8\n",
        "model.export(\n",
        "  MODEL_FILENAME,\n",
        "  input_image_shape=(input_height, input_width),\n",
        "  num_pre_nms_predictions=1000,\n",
        "  max_predictions_per_image=20,\n",
        "  nms_threshold=0.7,\n",
        "  confidence_threshold=0.4,\n",
        "  output_predictions_format=DetectionOutputFormatMode.FLAT_FORMAT,\n",
        "  quantization_mode=quantization_mode\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "dummy_input = np.random.randint(0, 255, (1, 3, input_width, input_height)).astype(np.uint8)\n",
        "\n",
        "ort_session = ort.InferenceSession(MODEL_FILENAME, providers=[\"ROCMExecutionProvider\"])\n",
        "ort_session.run(None, {\"input\": dummy_input})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOBHFhWVeNDi",
        "outputId": "3e428ff3-88a7-4e7a-eacd-c1dce32efcfd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:118: UserWarning: Specified provider 'ROCMExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.000000e+00,  9.965515e-02, -7.989502e-02,  3.195147e+02,\n",
              "          3.200067e+02,  8.423366e-01,  5.000000e+01]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uBhXV5g4Nh42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d402de37-bc73-4b51-bfba-9db30080c3db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a8db2696-bf17-4548-b022-a72cfcdd9f40\", \"yolonas_s_uint8_320x320.onnx\", 49372793)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(MODEL_FILENAME)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}